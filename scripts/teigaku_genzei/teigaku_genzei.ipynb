{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16b731d",
   "metadata": {},
   "source": [
    "# Teigaku Genzei (fixed-amount tax reduction) example\n",
    "\n",
    "## Preparation\n",
    "### Please read README.md and understand the licensing conditions and requirements on the source data distributed by National Tax Agency of Japan.\n",
    "- [README.md](./README.md)\n",
    "\n",
    "### Source data\n",
    "URLs of the source PDF files are also available in [README.md](./README.md). Please download those by yourself, and store those files in `source/` directory.\n",
    "- 0024001-021.pdf\n",
    "- 0024004-072_01.pdf\n",
    "\n",
    "### Prerequisites\n",
    "Please prepare a python virtual environment with the following dependencies.\n",
    "- Python >= 3.10\n",
    "- sdg_hub >= 0.7.1\n",
    "- notebook\n",
    "- ipykernel\n",
    "\n",
    "No GPU or external LLM inference service is required in this notebook.\n",
    "\n",
    "## Overview\n",
    "This notebook covers the following topics, each of which correspond to a section in [README.md](./README.md).\n",
    "- Common preprocessing of source document\n",
    "  - Extraction of text information from PDF source files by calling Docling.\n",
    "  - QA pair extraction.\n",
    "- SDG seed generation\n",
    "  - Construction of seed contexts.\n",
    "  - Construction of in-context learning examples.\n",
    "  - SDG seed generation.\n",
    "\n",
    "In README.md, the same things can be done with command-line python tools and shell scripts.\n",
    "For the details of the input / output files and the options of each step, please refer to README.md.\n",
    "\n",
    "Other topics such as synthetic training data generation, fine-tuning, and task-specific evaluation of models, please refer to README.md, as well as the sdg_hub and training_hub documents. \n",
    "You will need external LLM inference services or GPUs for the SDGs.\n",
    "You will need GPUs for the training and evaluation of models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a1dd96",
   "metadata": {},
   "source": [
    "## Configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6ef779",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_input_dir = \"source\"\n",
    "docparser_dir = \"docparser\"\n",
    "source_name_list = [\"0024001-021\", \"0024004-072_01\"]\n",
    "qa_table_dir = \"qa_table\"\n",
    "context_dir = \"context\"\n",
    "icl_source_name = source_name_list[1]\n",
    "icl_path = \"icl/icl.jsonl\"\n",
    "seed_source_name = source_name_list[0]\n",
    "seed_path = \"seed/seed_ja.jsonl\"\n",
    "\n",
    "# Docling configurations.\n",
    "# Constants and type definitions\n",
    "EXPORT_FORMATS = {\n",
    "    \"json\": (\"json\", \"export_to_dict\"),  # Deep Search JSON format\n",
    "    \"text\": (\"txt\", \"export_to_text\"),  # Plain text\n",
    "    \"markdown\": (\"md\", \"export_to_markdown\"),  # Markdown with structure\n",
    "    \"html\": (\"html\", \"export_to_html\"),  # HTML with styling\n",
    "    \"doctags\": (\"doctags\", \"export_to_document_tokens\"),  # Document tokens\n",
    "}\n",
    "\n",
    "DEFAULT_CONFIG = {\n",
    "    \"pipeline\": {\n",
    "        \"ocr\": {\n",
    "            \"enabled\": True,  # Enable/disable OCR processing\n",
    "            \"languages\": [\"es\"],  # List of language codes (e.g., eng, fra, deu)\n",
    "        },\n",
    "        \"tables\": {\n",
    "            \"enabled\": True,  # Enable/disable table detection\n",
    "            \"cell_matching\": True,  # Enable/disable cell matching in tables\n",
    "        },\n",
    "        \"performance\": {\n",
    "            \"threads\": 4,  # Number of processing threads\n",
    "            \"device\": \"auto\",  # Device selection (auto, cpu, gpu)\n",
    "        },\n",
    "    },\n",
    "    \"export\": {\n",
    "        \"formats\": {\n",
    "            \"json\": True,  # Deep Search JSON format\n",
    "            \"text\": True,  # Plain text\n",
    "            \"markdown\": True,  # Markdown with structure\n",
    "            \"html\": True,  # HTML with styling\n",
    "            \"doctags\": True,  # Document tokens\n",
    "        }\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c16b248",
   "metadata": {},
   "source": [
    "# Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c5426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import jsonl_util\n",
    "\n",
    "from sdg_hub.core.utils.logger_config import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e1b7e8",
   "metadata": {},
   "source": [
    "## Extraction of text information from PDF source files by calling Docling.\n",
    "\n",
    "The code in this block is a simplified version of [docparser_v2.py](https://github.com/Red-Hat-AI-Innovation-Team/sdg_hub/blob/main/examples/knowledge_tuning/instructlab/docparser_v2.py).\n",
    "\n",
    "The text information in the source PDF files are extracted using Docling.\n",
    "\n",
    "- Inputs: Source PDF files. These files contain FAQ and answers to the questions.\n",
    "- Outputs: Two sets of (.doctags, .html, .json, .md, .txt) files. We only use .json files in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standard\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Third Party\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    "    TableStructureOptions,\n",
    ")\n",
    "from docling.datamodel.accelerator_options import (\n",
    "    AcceleratorDevice,\n",
    "    AcceleratorOptions,\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "def setup_pipeline_options(config: dict) -> PdfPipelineOptions:\n",
    "    \"\"\"Configure pipeline options from config dictionary.\"\"\"\n",
    "    pipeline_config = config[\"pipeline\"]\n",
    "\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.do_ocr = pipeline_config[\"ocr\"][\"enabled\"]\n",
    "    pipeline_options.do_table_structure = pipeline_config[\"tables\"][\"enabled\"]\n",
    "    if isinstance(pipeline_options.table_structure_options, TableStructureOptions) :\n",
    "        pipeline_options.table_structure_options.do_cell_matching = pipeline_config[\n",
    "            \"tables\"\n",
    "        ][\"cell_matching\"]\n",
    "    pipeline_options.ocr_options.lang = pipeline_config[\"ocr\"][\"languages\"]\n",
    "    pipeline_options.accelerator_options = AcceleratorOptions(\n",
    "        num_threads=pipeline_config[\"performance\"][\"threads\"],\n",
    "        device=getattr(\n",
    "            AcceleratorDevice, pipeline_config[\"performance\"][\"device\"].upper()\n",
    "        ),\n",
    "    )\n",
    "    return pipeline_options\n",
    "\n",
    "\n",
    "def export_document(\n",
    "    conv_result, doc_filename: str, output_dir: Path, config: dict\n",
    ") -> None:\n",
    "    \"\"\"Export document in configured formats.\"\"\"\n",
    "    enabled_formats = {\n",
    "        k: v\n",
    "        for k, v in EXPORT_FORMATS.items()\n",
    "        if config[\"export\"][\"formats\"].get(k, True)\n",
    "    }\n",
    "\n",
    "    for format_name, (extension, export_method) in enabled_formats.items():\n",
    "        try:\n",
    "            content = getattr(conv_result.document, export_method)()\n",
    "            output_path = output_dir / f\"{doc_filename}.{extension}\"\n",
    "\n",
    "            with output_path.open(\"w\", encoding=\"utf-8\") as fp:\n",
    "                if isinstance(content, (dict, list)):\n",
    "                    json.dump(content, fp, ensure_ascii=False, indent=2)\n",
    "                else:\n",
    "                    fp.write(content)\n",
    "\n",
    "            logger.debug(f\"Successfully exported {format_name} format to {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to export {format_name} format: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def export_document_new_docling(\n",
    "    input_dir: Path,\n",
    "    output_dir: Path,\n",
    "    # config: Optional[Path],\n",
    "):\n",
    "    \"\"\"Convert PDF documents and export them in multiple formats.\"\"\"\n",
    "    # config_data = load_config(config)\n",
    "    config_data = DEFAULT_CONFIG\n",
    "\n",
    "    file_paths = list(input_dir.glob(\"*.pdf\"))\n",
    "    if not file_paths:\n",
    "        logger.warning(f\"No PDF files found in {input_dir}\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Found {len(file_paths)} PDF files to process\")\n",
    "\n",
    "    pipeline_options = setup_pipeline_options(config_data)\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    success_count = failure_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        logger.info(f\"Processing {file_path}\")\n",
    "        try:\n",
    "            conv_result = doc_converter.convert(file_path)\n",
    "            doc_filename = conv_result.input.file.stem\n",
    "\n",
    "            export_document(conv_result, doc_filename, output_dir, config_data)\n",
    "            success_count += 1\n",
    "            logger.info(f\"Successfully processed {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            failure_count += 1\n",
    "            logger.error(f\"Failed to process {file_path}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    processing_time = time.time() - start_time\n",
    "\n",
    "    logger.info(\n",
    "        f\"Processed {success_count + failure_count} docs in {processing_time:.2f} seconds\"\n",
    "        f\"\\n  Successful: {success_count}\"\n",
    "        f\"\\n  Failed: {failure_count}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eba96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_document_new_docling(input_dir=Path(source_input_dir), output_dir=Path(docparser_dir))\n",
    "logger.info(f\"Done! Please check the outputs in {docparser_dir}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30766d6e",
   "metadata": {},
   "source": [
    "## QA pair extraction.\n",
    "\n",
    "The QA pairs in the JSON files from the previous step are extracted using a python script.\n",
    "The extracted QA pairs are used for test cases in the evaluation, as well as for In-context learning examples and seed contexts in the later sections.\n",
    "\n",
    "The glossary sections (i.e., term - definition pairs) in the same files are also extracted similarly.\n",
    "The glossary files are currently not used, but reserved for future extension.\n",
    "\n",
    "- Inputs: JSON files that are outputs of Docling.\n",
    "- Outputs: A set of two CSV files, one is for the QA pairs (\\*.csv) and another one is for the glossary (\\*_glossary.csv).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7460f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json_qa\n",
    "import json_glossary\n",
    "import json_util\n",
    "\n",
    "def make_common_fn(docparser_dir: str, source_name_list: list[str], qa_table_dir: str):\n",
    "    for source_name in source_name_list:\n",
    "        in_file = os.path.join(docparser_dir, source_name + \".json\")\n",
    "        out_file = os.path.join(qa_table_dir, source_name + \".csv\")\n",
    "        out_glossary_file = os.path.join(qa_table_dir, source_name + \"_glossary.csv\")\n",
    "\n",
    "        data = json_util.read_json_file(in_file)\n",
    "\n",
    "        qa_pairs = json_qa.extract_qa_pairs(data)\n",
    "        json_qa.save_to_csv(qa_pairs, out_file)\n",
    "\n",
    "        qa_glossary_pairs = json_glossary.extract_glossary(data)\n",
    "        json_glossary.save_to_csv(qa_glossary_pairs, out_glossary_file)\n",
    "\n",
    "make_common_fn(\n",
    "    docparser_dir=docparser_dir,\n",
    "    source_name_list=source_name_list,\n",
    "    qa_table_dir=qa_table_dir\n",
    ")\n",
    "logger.info(f\"Done! Please check the outputs in {qa_table_dir}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83415a9",
   "metadata": {},
   "source": [
    "## Construction of seed contexts.\n",
    "\n",
    "A context is a chunk of text which contains the knowledge to be taught to the student model.\n",
    "Since the source files are FAQ documents, we compose contexts by concatenating the answers of the QA pairs.\n",
    "The questions are not used in this step.\n",
    "\n",
    "- Inputs: CSV files of the QA pairs from the previous step.\n",
    "- Outputs: CSV files of the contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be7de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import make_context\n",
    "\n",
    "def make_context_fn(qa_table_dir: str, source_name_list: list[str], context_dir):\n",
    "    for source_name in source_name_list:\n",
    "        input_qa_path = os.path.join(qa_table_dir, source_name + \".csv\")\n",
    "        input_glossary_path = os.path.join(qa_table_dir, source_name + \"_glossary.csv\")\n",
    "        output_context_path = os.path.join(context_dir, source_name + \"_context.csv\")\n",
    "\n",
    "        qa_df = pd.read_csv(input_qa_path, encoding=\"utf8\")\n",
    "        glossary_df = pd.read_csv(input_glossary_path, encoding=\"utf8\")\n",
    "\n",
    "        context_df = make_context.make_context(qa_df, glossary_df, make_context.OPT_GLOSSARY_APPENDIX)\n",
    "\n",
    "        context_df.to_csv(output_context_path, index=False, encoding=\"utf8\")\n",
    "\n",
    "make_context_fn(\n",
    "    qa_table_dir=qa_table_dir,\n",
    "    source_name_list=source_name_list,\n",
    "    context_dir=context_dir\n",
    ")\n",
    "logger.info(f\"Done! Please check the outputs in {context_dir}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa594c52",
   "metadata": {},
   "source": [
    "## Construction of in-context learning examples.\n",
    "\n",
    "In-context learning examples are used to exemplify how to synthesize QA pairs from a context to the teacher model.\n",
    "In sdg_hub, an ICL example consists of three QA pairs and one context that is related to the QAs.\n",
    "We compose ICL examples from one source, 0024004-072_01.pdf.\n",
    "The other source is used for the seed contexts.\n",
    "\n",
    "- Input: \n",
    "  - CSV file of QA pairs extracted from the source for ICL (see above).\n",
    "  - CSV file of contexts extracted from the source for ICL (see above).\n",
    "- Output:\n",
    "  - ICL file in JSONL format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f1c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import make_icl\n",
    "\n",
    "def make_icl_fn(input_qa_path: str, input_context_path: str, output_icl_path: str, short_context: bool):\n",
    "    qa_df = pd.read_csv(input_qa_path, encoding=\"utf8\")\n",
    "    context_df = pd.read_csv(input_context_path, encoding=\"utf8\")\n",
    "\n",
    "    icl_list = make_icl.make_icl(qa_df, context_df, short_context)\n",
    "\n",
    "    jsonl_util.write_jsonl_file(output_icl_path, icl_list)\n",
    "\n",
    "make_icl_fn(\n",
    "    input_qa_path=os.path.join(qa_table_dir, icl_source_name + \".csv\"),\n",
    "    input_context_path=os.path.join(context_dir, icl_source_name + \"_context.csv\"),\n",
    "    output_icl_path=icl_path,\n",
    "    short_context=True\n",
    ")\n",
    "logger.info(f\"Done! Please check the outputs in {icl_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113ce71",
   "metadata": {},
   "source": [
    "## SDG seed generation.\n",
    "\n",
    "The SDG seed data is the input to sdg_hub, where the seed contexts are converted into\n",
    "a set of question-answer pairs synthesized by a teacher model.\n",
    "\n",
    "Here, we use the other source, 0024001-021.pdf, for the seed context data. \n",
    "These are combined with the ICL examples in the output.\n",
    "\n",
    "Note that `OPT_JOIN_METHOD_CARTESIAN` option specifies the function to\n",
    "generate all the possible combinations of ICL examples and seed contexts to \n",
    "diversify the synthesis of the QA pairs.\n",
    "For example, if we have 4 ICL examples and 7 seed contexts, there will be 4 x 7 = 28 patterns of SDG seeds.\n",
    "\n",
    "- Inputs: \n",
    "  - CSV file of the seed contexts.\n",
    "  - JSONL file of the ICL examples.\n",
    "- Outputs: \n",
    "  - JSONL file of the SDG seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b681b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import make_seed\n",
    "\n",
    "def make_seed_fn(input_context_path: str, input_icl_path: str, output_seed_path: str, opt_join_method: str):\n",
    "    context_df = pd.read_csv(input_context_path, encoding=\"utf8\")[[\"context\", \"qindex\"]]\n",
    "    icl_list = jsonl_util.read_jsonl_file(input_icl_path)\n",
    "    out_df = make_seed.make_seed(context_df, icl_list, opt_join_method)\n",
    "    \n",
    "    out_df.to_json(output_seed_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "make_seed_fn(\n",
    "    input_context_path=os.path.join(context_dir, seed_source_name + \"_context.csv\"),\n",
    "    input_icl_path=icl_path,\n",
    "    output_seed_path=seed_path,\n",
    "    opt_join_method=make_seed.OPT_JOIN_METHOD_CARTESIAN\n",
    ")\n",
    "logger.info(f\"Done! Please check the outputs in {seed_path}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdg_hub_doc2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
